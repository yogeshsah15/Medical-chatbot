%pwd
import os
os.chdir("../")
from langchain.document_loaders import PyPDFLoader, DirectoryLoader 
from langchain.text_splitter import RecursiveCharacterTextSplitter 
# Extract data from pdf file
def load_pdf_file(data):
    loader=DirectoryLoader(data,glob="*.pdf",loader_cls=PyPDFLoader)
    
    documents=loader.load()

    return documents
extracted_data = load_pdf_file(data='C:/Users/KIIT01/Projects/Medical-chatbot/Data')
def text_split(extracted_data):
    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)
    text_chunks=text_splitter.split_documents(extracted_data)
    return text_chunks  

text_chunks=text_split(extracted_data)
print("Length of Text Chunks", len(text_chunks))
from langchain_huggingface import HuggingFaceEmbeddings
def download_hugging_face_embeddings():
    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    return embeddings
embeddings = download_hugging_face_embeddings()

query_result = embeddings.embed_query("hello")
print("Length" , len(query_result))
from dotenv import load_dotenv
load_dotenv()
import os
PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')
GROQ_API_KEY=os.environ.get('GROQ_API_KEY')
from pinecone.grpc import PineconeGRPC as Pinecone
from pinecone import ServerlessSpec
import os

pc = Pinecone(api_key=PINECONE_API_KEY)

index_name= "medichatbot"


pc.create_index(
    name=index_name,
    dimension=384,
    metric="cosine",
    spec=ServerlessSpec(
        cloud="aws",
        region="us-east-1"
        
    )

)
import os
os.environ["PINECONE_API_KEY"] = PINECONE_API_KEY
os.environ["GROQ_API_KEY"] = GROQ_API_KEY

from langchain_pinecone import PineconeVectorStore 

docsearch = PineconeVectorStore.from_documents(
    documents=text_chunks,
    index_name=index_name,
    embedding=embeddings,
)


#existing index
from langchain_pinecone import PineconeVectorStore
docsearch = PineconeVectorStore.from_existing_index(
    index_name=index_name,
    embedding=embeddings
)

docsearch

retriever = docsearch.as_retriever(search_type="similarity",search_kwargs={"k":3})
retrieved_docs = retriever.invoke("What is warts?")

from langchain_groq import ChatGroq
llm = ChatGroq(model="llama3-70b-8192", temperature=0.4, max_tokens=500)
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate



system_prompt = (
    "You are an assitant for question-answering tasks."
    "Use the following pieces of retrieved context to answer"
    "the question. If you don't know thw answer,say that you"
    " don't know . Use three sentences maximum and keep the " 
    "answer concise."
    "\n\n"
    "{context}"       
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

response = rag_chain.invoke({"input": "Acne?"})
print(response["answer"])  # or just `print(response)` depending on the return format

